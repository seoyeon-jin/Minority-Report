"""
í•™ìŠµëœ FOCAL ëª¨ë¸ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµ ì™„ë£Œ í›„ checkpointì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
"""
import torch
import sys
from pathlib import Path

# FOCAL í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from demo_visualization import FOCALVisualizerDemo


def visualize_checkpoint(checkpoint_path, dataloader, device='cuda', save_path='trained_model_analysis.png'):
    """
    í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ì‹œê°í™”
    
    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: 'checkpoints/best_model.pth')
        dataloader: Validation ë˜ëŠ” Test DataLoader
        device: 'cuda' ë˜ëŠ” 'cpu'
        save_path: ì‹œê°í™” ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    """
    print("\n" + "="*60)
    print("ğŸ¨ í•™ìŠµëœ FOCAL ëª¨ë¸ ì‹œê°í™”")
    print("="*60)
    
    # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    print(f"\nğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
    if 'epoch' in checkpoint:
        print(f"   - Epoch: {checkpoint['epoch']}")
    if 'best_acc' in checkpoint:
        print(f"   - Best Accuracy: {checkpoint['best_acc']:.4f}")
    
    # 2. ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ)
    print("\nâš ï¸  ì£¼ì˜: ì•„ë˜ ëª¨ë¸ ë¡œë“œ ì½”ë“œë¥¼ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!")
    print("""
    # ì˜ˆì‹œ ì½”ë“œ:
    from src.models.FOCALModules import FOCAL
    from src.models.DeepSense import DeepSense  # ë˜ëŠ” ë‹¤ë¥¸ ë°±ë³¸
    
    # ë°±ë³¸ ìƒì„±
    backbone = DeepSense(args)
    
    # FOCAL ëª¨ë¸ ìƒì„±
    model = FOCAL(args, backbone)
    
    # State dict ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    """)
    
    # ë°ëª¨ìš© ë”ë¯¸ ëª¨ë¸ (ì‹¤ì œ ì‚¬ìš© ì‹œ ìœ„ ì½”ë“œë¡œ êµì²´)
    print("\nâš ï¸  í˜„ì¬ëŠ” ë”ë¯¸ ë°ì´í„°ë¡œ ë°ëª¨ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    print("    ì‹¤ì œ ì‚¬ìš© ì‹œ ìœ„ ì£¼ì„ì„ í•´ì œí•˜ê³  ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.\n")
    
    # 3. ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    batch = next(iter(dataloader))
    
    # 4. Forward pass (ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ)
    print("ğŸ”® Forward pass...")
    print("""
    # ì˜ˆì‹œ ì½”ë“œ:
    with torch.no_grad():
        # ì…ë ¥ ì¤€ë¹„
        aug_freq_input1 = batch['aug1'].to(device)
        aug_freq_input2 = batch['aug2'].to(device)
        
        # Forward pass
        mod_features1, mod_features2 = model(
            aug_freq_input1,
            aug_freq_input2,
            proj_head=False  # ì¤‘ìš”: projection head ì „ì˜ features í•„ìš”
        )
        
        # ì²« ë²ˆì§¸ augmentationì˜ features ì‚¬ìš©
        mod_features = mod_features1
    """)
    
    # ë°ëª¨ìš© ë”ë¯¸ features (ì‹¤ì œ ì‚¬ìš© ì‹œ ìœ„ ì½”ë“œë¡œ êµì²´)
    from demo_visualization import create_dummy_features
    print("âš ï¸  ë”ë¯¸ features ìƒì„± ì¤‘...")
    mod_features = create_dummy_features(batch_size=64, seq_len=4, feature_dim=256)
    
    # 5. ì‹œê°í™”
    print("\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
    visualizer = FOCALVisualizerDemo(figsize=(20, 12))
    result_path = visualizer.visualize_all(mod_features, save_path)
    
    print("\n" + "="*60)
    print("âœ… ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {result_path}")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. ìœ„ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œë¥¼ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •")
    print("   2. ì²´í¬í¬ì¸íŠ¸ì™€ ë°ì´í„°ë¡œë” ê²½ë¡œ ì„¤ì •")
    print("   3. ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰")
    print("\n" + "="*60)
    
    return result_path


def visualize_during_training_example():
    """
    í•™ìŠµ ë£¨í”„ì— í†µí•©í•˜ëŠ” ì˜ˆì‹œ ì½”ë“œ
    """
    print("\n" + "="*60)
    print("ğŸ“š í•™ìŠµ ì¤‘ ì‹œê°í™” í†µí•© ì˜ˆì‹œ")
    print("="*60)
    
    code = '''
# focal/src/train_utils/pretrain.py ë˜ëŠ” finetune.py ìˆ˜ì •

from demo_visualization import FOCALVisualizerDemo
import os

# í•™ìŠµ ì‹œì‘ ì „ì— ì´ˆê¸°í™”
visualizer = FOCALVisualizerDemo(figsize=(20, 12))
vis_dir = 'visualizations'
os.makedirs(vis_dir, exist_ok=True)

# í•™ìŠµ ë£¨í”„
for epoch in range(args.dataset_config[args.learn_framework]["pretrain_lr_scheduler"]["train_epochs"]):
    
    # ... í•™ìŠµ ì½”ë“œ ...
    model.train()
    for batch in train_loader:
        # í•™ìŠµ ì§„í–‰
        loss = criterion(...)
        optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        # ... Validation ì½”ë“œ ...
        
        # âœ¨ ë§¤ 10 epochë§ˆë‹¤ ì‹œê°í™” (ë˜ëŠ” ì›í•˜ëŠ” ì£¼ê¸°)
        if epoch % 10 == 0:
            print(f"\\nğŸ¨ Epoch {epoch}: ì‹œê°í™” ìƒì„± ì¤‘...")
            
            # Validation batchì—ì„œ features ì¶”ì¶œ
            val_batch = next(iter(val_loader))
            aug1 = val_batch['aug1'].to(device)
            aug2 = val_batch['aug2'].to(device)
            
            # Forward pass
            mod_features, _ = model(aug1, aug2, proj_head=False)
            
            # ì‹œê°í™” ì €ì¥
            save_path = os.path.join(vis_dir, f'epoch_{epoch:04d}.png')
            visualizer.visualize_all(mod_features, save_path)
            print(f"   âœ“ ì €ì¥: {save_path}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if epoch % 50 == 0:
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_acc': best_acc,
        }, f'checkpoints/epoch_{epoch:04d}.pth')

print("\\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"ì‹œê°í™” ê²°ê³¼: {vis_dir}/ í´ë” í™•ì¸")
'''
    
    print(code)
    print("\n" + "="*60)


def quick_visualization_guide():
    """ë¹ ë¥¸ ì‚¬ìš© ê°€ì´ë“œ"""
    print("\n" + "ğŸš€" * 30)
    print("ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ")
    print("ğŸš€" * 30)
    
    print("\nğŸ“ Case 1: í•™ìŠµ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„")
    print("-" * 60)
    print("""
from visualize_trained_model import visualize_checkpoint

# ì²´í¬í¬ì¸íŠ¸ì™€ ë°ì´í„°ë¡œë” ì¤€ë¹„
checkpoint_path = 'checkpoints/best_model.pth'
val_loader = create_your_val_dataloader()  # í”„ë¡œì íŠ¸ì˜ dataloader

# ì‹œê°í™” ì‹¤í–‰
visualize_checkpoint(
    checkpoint_path=checkpoint_path,
    dataloader=val_loader,
    device='cuda',
    save_path='final_analysis.png'
)
""")
    
    print("\nğŸ“ Case 2: í•™ìŠµ ì¤‘ ì£¼ê¸°ì  ì‹œê°í™”")
    print("-" * 60)
    print("""
from demo_visualization import FOCALVisualizerDemo

visualizer = FOCALVisualizerDemo()

# í•™ìŠµ ë£¨í”„ ë‚´ë¶€
for epoch in range(epochs):
    # ... í•™ìŠµ ...
    
    if epoch % 10 == 0:  # ë§¤ 10 epoch
        with torch.no_grad():
            mod_features, _ = model(val_batch1, val_batch2, proj_head=False)
            visualizer.visualize_all(mod_features, f'epoch_{epoch}.png')
""")
    
    print("\nğŸ“ Case 3: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ë¹„êµ")
    print("-" * 60)
    print("""
import glob

checkpoints = sorted(glob.glob('checkpoints/epoch_*.pth'))

for ckpt in checkpoints:
    epoch = int(ckpt.split('_')[-1].split('.')[0])
    
    # ê° ì²´í¬í¬ì¸íŠ¸ ì‹œê°í™”
    visualize_checkpoint(
        checkpoint_path=ckpt,
        dataloader=val_loader,
        save_path=f'comparison/epoch_{epoch:04d}.png'
    )

print("âœ“ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì‹œê°í™” ì™„ë£Œ!")
""")
    
    print("\n" + "="*60)


if __name__ == '__main__':
    print("\n" + "ğŸ¯" * 30)
    print("í•™ìŠµëœ FOCAL ëª¨ë¸ ì‹œê°í™” ë„êµ¬")
    print("ğŸ¯" * 30)
    
    # ì‚¬ìš© ê°€ì´ë“œ ì¶œë ¥
    quick_visualization_guide()
    
    print("\n" + "="*60)
    print("ğŸ’¡ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ")
    print("="*60)
    print("""
# 1. ì´ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •:
#    - ëª¨ë¸ ë¡œë“œ ì½”ë“œ ì£¼ì„ í•´ì œ
#    - í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •

# 2. ì‹¤í–‰:
from visualize_trained_model import visualize_checkpoint

checkpoint_path = 'checkpoints/best_model.pth'
val_loader = your_val_dataloader  # ì‹¤ì œ ë°ì´í„°ë¡œë”

visualize_checkpoint(checkpoint_path, val_loader)

# 3. ê²°ê³¼ í™•ì¸:
#    trained_model_analysis.png íŒŒì¼ ìƒì„±ë¨!
""")
    
    print("\ní•™ìŠµ ì¤‘ í†µí•© ì˜ˆì‹œ:")
    visualize_during_training_example()
    
    print("\n" + "="*60)
    print("âœ… ì¤€ë¹„ ì™„ë£Œ! ìœ„ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")
    print("="*60)

