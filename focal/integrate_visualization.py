"""
FOCAL í•™ìŠµ ì½”ë“œì— ì‹œê°í™” í†µí•©í•˜ê¸°
ì‹¤ì œ í•™ìŠµëœ hidden stateë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
"""

# ============================================================
# ğŸ“ ìœ„ì¹˜ 1: focal/src/train_utils/pretrain.py
# ============================================================

PRETRAIN_INTEGRATION = """
# focal/src/train_utils/pretrain.py íŒŒì¼ì˜ ìƒë‹¨ì— ì¶”ê°€

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from demo_visualization import FOCALVisualizerDemo

def pretrain(
    args,
    backbone_model,
    augmenter,
    train_dataloader,
    val_dataloader,
    test_dataloader,
    loss_func,
    num_batches,
):
    # ê¸°ì¡´ ì½”ë“œ...
    default_model = init_pretrain_framework(args, backbone_model)
    optimizer = define_optimizer(args, default_model.parameters())
    lr_scheduler = define_lr_scheduler(args, optimizer)
    
    # âœ¨ ì‹œê°í™” ì´ˆê¸°í™” ì¶”ê°€
    visualizer = FOCALVisualizerDemo(figsize=(20, 12))
    vis_dir = os.path.join(args.output_folder, 'visualizations')
    os.makedirs(vis_dir, exist_ok=True)
    logging.info(f"ì‹œê°í™” ì €ì¥ ê²½ë¡œ: {vis_dir}")
    
    # Training loop
    for epoch in range(args.dataset_config[args.learn_framework]["pretrain_lr_scheduler"]["train_epochs"]):
        
        # ... í•™ìŠµ ì½”ë“œ ...
        default_model.train()
        train_loss_list = []
        
        for i, (time_loc_inputs, _) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
            optimizer.zero_grad()
            loss = calc_pretrain_loss(args, default_model, augmenter, loss_func, time_loc_inputs)
            loss.backward()
            optimizer.step()
            train_loss_list.append(loss.item())
        
        # âœ¨ ë§¤ 10 epochë§ˆë‹¤ ì‹œê°í™” ì¶”ê°€
        if epoch % 10 == 0:
            # KNN validation (ê¸°ì¡´ ì½”ë“œ)
            knn_estimator = compute_knn(args, default_model.backbone, augmenter, train_dataloader)
            
            train_loss = np.mean(train_loss_list)
            val_acc, val_loss = val_and_logging(...)
            
            # âœ¨âœ¨âœ¨ ì‹œê°í™” ì¶”ê°€ âœ¨âœ¨âœ¨
            logging.info(f"Epoch {epoch}: ì‹œê°í™” ìƒì„± ì¤‘...")
            try:
                # Validation batch ê°€ì ¸ì˜¤ê¸°
                val_iter = iter(val_dataloader)
                val_batch, _ = next(val_iter)
                
                # Augmentation
                aug_freq_loc_inputs_1 = augmenter.forward("fixed", val_batch)
                aug_freq_loc_inputs_2 = augmenter.forward("fixed", val_batch)
                
                # Forward passë¡œ features ì¶”ì¶œ (proj_head=False!)
                default_model.eval()
                with torch.no_grad():
                    mod_features, _ = default_model(
                        aug_freq_loc_inputs_1,
                        aug_freq_loc_inputs_2,
                        proj_head=False  # âš ï¸ ì¤‘ìš”: projection head ì „ì˜ features
                    )
                
                # ì‹œê°í™” ì €ì¥
                save_path = os.path.join(vis_dir, f'epoch_{epoch:04d}.png')
                visualizer.visualize_all(mod_features, save_path)
                logging.info(f"   âœ“ ì‹œê°í™” ì €ì¥: {save_path}")
                
            except Exception as e:
                logging.warning(f"   âœ— ì‹œê°í™” ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ ë‹¤ì‹œ train modeë¡œ
            default_model.train()
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ê¸°ì¡´ ì½”ë“œ)
            torch.save(default_model.backbone.state_dict(), latest_weight)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(default_model.backbone.state_dict(), best_weight)
        
        lr_scheduler.step(epoch)
    
    logging.info(f"âœ… í•™ìŠµ ì™„ë£Œ! ì‹œê°í™” ê²°ê³¼: {vis_dir}")
"""


# ============================================================
# ğŸ“ ìœ„ì¹˜ 2: Loss ê°’ í™•ì¸ (ì„ íƒì‚¬í•­)
# ============================================================

LOSS_LOGGING = """
# focal/src/models/loss.pyì˜ FOCALLoss.forward() ë©”ì„œë“œ ëë¶€ë¶„

def forward(self, mod_features1, mod_features2, index=None):
    # ... ê¸°ì¡´ loss ê³„ì‚° ì½”ë“œ ...
    
    # Step 2: shared space contrastive loss
    shared_contrastive_loss = 0
    # ... ê³„ì‚° ...
    
    # Step 3: private space contrastive loss
    private_contrastive_loss = 0
    # ... ê³„ì‚° ...
    
    # Step 4: temporal consistency loss
    temporal_consistency_loss = 0
    # ... ê³„ì‚° ...
    
    # Step 5: orthogonality loss
    orthogonality_loss = 0
    # ... ê³„ì‚° ...
    
    loss = (
        shared_contrastive_loss * self.config["shared_contrastive_loss_weight"]
        + private_contrastive_loss * self.config["private_contrastive_loss_weight"]
        + orthogonality_loss * self.config["orthogonal_loss_weight"]
        + temporal_consistency_loss * self.config["rank_loss_weight"]
    )
    
    # âœ¨ Loss ê°’ë“¤ì„ dictë¡œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • (ë””ë²„ê¹…ìš©)
    loss_dict = {
        'total': loss.item(),
        'shared': shared_contrastive_loss.item(),
        'private': private_contrastive_loss.item(),
        'orthogonal': orthogonality_loss.item(),
        'temporal': temporal_consistency_loss.item(),
    }
    
    # ê¸°ì¡´: return loss
    # ìˆ˜ì •: return loss, loss_dict  # dictë„ í•¨ê»˜ ë°˜í™˜
    return loss  # ë˜ëŠ” lossì™€ dict ë‘˜ ë‹¤ ë°˜í™˜
"""


# ============================================================
# ğŸ“ ì‹¤ì œ ì ìš© ë°©ë²•
# ============================================================

def print_integration_guide():
    """í†µí•© ê°€ì´ë“œ ì¶œë ¥"""
    
    print("\n" + "="*80)
    print("ğŸ¨ FOCAL ì‹œê°í™” í†µí•© ê°€ì´ë“œ")
    print("="*80)
    
    print("\nğŸ“ Step 1: pretrain.py ìˆ˜ì •")
    print("-" * 80)
    print("íŒŒì¼: focal/src/train_utils/pretrain.py")
    print("\nìˆ˜ì • ìœ„ì¹˜:")
    print("  1) íŒŒì¼ ìƒë‹¨ì— import ì¶”ê°€")
    print("  2) pretrain() í•¨ìˆ˜ ë‚´ë¶€ì— visualizer ì´ˆê¸°í™”")
    print("  3) epoch ë£¨í”„ì— ì‹œê°í™” ì½”ë“œ ì¶”ê°€ (epoch % 10 == 0 ë¶€ë¶„)")
    
    print("\n" + PRETRAIN_INTEGRATION)
    
    print("\n" + "="*80)
    print("ğŸ“ Step 2: ì‹¤í–‰")
    print("-" * 80)
    print("""
# í•™ìŠµ ì‹œì‘
python focal/src/train.py \\
    --dataset MOD \\
    --model DeepSense \\
    --learn_framework FOCAL \\
    --stage pretrain

# í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ìƒì„±ë¨:
# - visualizations/epoch_0000.png
# - visualizations/epoch_0010.png
# - visualizations/epoch_0020.png
# - ...
""")
    
    print("\n" + "="*80)
    print("ğŸ“ Step 3: ê²°ê³¼ í™•ì¸")
    print("-" * 80)
    print("""
# ìƒì„±ëœ ì‹œê°í™” í™•ì¸
ls -lh output/MOD_DeepSense_pretrain/visualizations/

# íŠ¹ì • epoch í™•ì¸
open output/MOD_DeepSense_pretrain/visualizations/epoch_0100.png

# GIFë¡œ ë³€í™˜ (ì„ íƒì‚¬í•­)
cd output/MOD_DeepSense_pretrain/visualizations/
convert -delay 50 -loop 0 epoch_*.png training_progress.gif
""")
    
    print("\n" + "="*80)
    print("ğŸ“ í•µì‹¬ í¬ì¸íŠ¸")
    print("-" * 80)
    print("""
âœ… Features ì¶”ì¶œ:
   mod_features, _ = default_model(aug1, aug2, proj_head=False)
   
   âš ï¸ proj_head=Falseê°€ ì¤‘ìš”! 
      True: projection head ê±°ì¹œ features (loss ê³„ì‚°ìš©)
      False: ì›ë³¸ backbone features (ì‹œê°í™”ìš©)

âœ… mod_features êµ¬ì¡°:
   {
       'seismic': tensor(batch, seq, dim),
       'audio': tensor(batch, seq, dim)
   }
   
   - ì´ê²Œ ë°”ë¡œ shared/privateë¡œ splitë˜ëŠ” features!
   - split_features()ë¡œ ë°˜ìœ¼ë¡œ ë‚˜ëˆ”
   - ì• ì ˆë°˜: shared, ë’¤ ì ˆë°˜: private

âœ… Loss ìœ„ì¹˜:
   focal/src/models/loss.pyì˜ FOCALLoss.forward()
   
   - shared_contrastive_loss
   - private_contrastive_loss
   - orthogonality_loss
   - temporal_consistency_loss
""")
    
    print("\n" + "="*80)
    print("ğŸ”§ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
    print("-" * 80)
    print("""
# 1. demoë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
cd /Users/sheoyonjin/Desktop/Minority-Report/focal
python demo_visualization.py

# 2. ìœ„ pretrain.py ìˆ˜ì • ì ìš©

# 3. í•™ìŠµ ì‹œì‘
python src/train.py --dataset MOD --learn_framework FOCAL --stage pretrain

# 4. í•™ìŠµ ì¤‘ visualizations/ í´ë” í™•ì¸
watch -n 10 ls -lh output/*/visualizations/
""")
    
    print("\n" + "="*80)


# ============================================================
# ğŸ“ ê°„ë‹¨ ë²„ì „ (ë³µì‚¬-ë¶™ì—¬ë„£ê¸°ìš©)
# ============================================================

SIMPLE_VERSION = """
# ===============================================
# pretrain.py ì— ì´ ì½”ë“œë§Œ ì¶”ê°€í•˜ë©´ ë!
# ===============================================

# 1) íŒŒì¼ ìƒë‹¨
from demo_visualization import FOCALVisualizerDemo
visualizer = FOCALVisualizerDemo()
vis_dir = 'visualizations'
os.makedirs(vis_dir, exist_ok=True)

# 2) epoch ë£¨í”„ ë‚´ë¶€ (epoch % 10 == 0 ë¶€ë¶„)
if epoch % 10 == 0:
    # ê¸°ì¡´ validation ì½”ë“œ...
    
    # ì‹œê°í™” ì¶”ê°€ (5ì¤„ë§Œ!)
    val_batch, _ = next(iter(val_dataloader))
    aug1 = augmenter.forward("fixed", val_batch)
    aug2 = augmenter.forward("fixed", val_batch)
    with torch.no_grad():
        features, _ = default_model(aug1, aug2, proj_head=False)
    visualizer.visualize_all(features, f'{vis_dir}/epoch_{epoch:04d}.png')
"""


if __name__ == '__main__':
    print_integration_guide()
    
    print("\n" + "ğŸš€" * 40)
    print("ê°„ë‹¨ ë²„ì „ (ë³µì‚¬-ë¶™ì—¬ë„£ê¸°)")
    print("ğŸš€" * 40)
    print(SIMPLE_VERSION)
    
    print("\n" + "="*80)
    print("âœ… ì¤€ë¹„ ì™„ë£Œ!")
    print("="*80)
    print("\nìœ„ ì½”ë“œë¥¼ focal/src/train_utils/pretrain.pyì— ì¶”ê°€í•˜ì„¸ìš”.")
    print("í•™ìŠµ ì‹œì‘í•˜ë©´ ìë™ìœ¼ë¡œ ì‹œê°í™”ê°€ ìƒì„±ë©ë‹ˆë‹¤! ğŸ¨")
    print("\n" + "="*80)

