"""Time-MMD ë°ì´í„°ì…‹ ë¡œë” (ê°œì„  ë²„ì „ - í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨)"""
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from sklearn.preprocessing import StandardScaler


class TimeMMDDatasetV2(Dataset):
    """
    Time-MMD ë©€í‹°ëª¨ë‹¬ ì‹œê³„ì—´ ë°ì´í„°ì…‹ (ê°œì„  ë²„ì „)
    
    Returns:
        xA: (T, dA) - numerical ì‹œê³„ì—´
        xB: (T, dB) - textual ì‹œê³„ì—´ (ì„ë² ë”© ë˜ëŠ” í†µê³„ê°’)
        maskA: (T,) - numerical ë§ˆìŠ¤í¬
        maskB: (T,) - textual ë§ˆìŠ¤í¬
        dates: List[datetime] - ê° ì‹œì ì˜ ë‚ ì§œ
        texts: List[str] - ê° ì‹œì ì˜ ì›ë³¸ í…ìŠ¤íŠ¸
        numerical_raw: (T, dA) - í‘œì¤€í™” ì „ numerical ê°’
    """
    
    def __init__(self, domain, window_size=128, stride=32, 
                 split='train', split_ratio=(0.6, 0.2, 0.2), normalize=True,
                 root_dir='.', text_mode='simple', return_metadata=True):
        """
        Args:
            domain: 'Agriculture', 'Climate', etc.
            window_size: ìœˆë„ìš° ê¸¸ì´
            stride: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° stride
            split: 'train', 'val', 'test'
            split_ratio: (train, val, test) ë¹„ìœ¨
            normalize: í‘œì¤€í™” ì—¬ë¶€
            root_dir: ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            text_mode: 'simple' (ê¸¸ì´/ë‹¨ì–´ìˆ˜), 'bert', 'sentence-transformer'
            return_metadata: dates, texts ë“± ë©”íƒ€ë°ì´í„° ë°˜í™˜ ì—¬ë¶€
        """
        self.domain = domain
        self.window_size = window_size
        self.stride = stride
        self.split = split
        self.normalize = normalize
        self.text_mode = text_mode
        self.return_metadata = return_metadata
        
        # Text encoder ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        self._init_text_encoder()
        
        # ë°ì´í„° ë¡œë“œ
        self.numerical_data, self.textual_data = self._load_data(root_dir, domain)
        
        # ì‹œê°„ì¶• ì •ë ¬
        self.aligned_data = self._align_timeseries()
        
        # Train/Val/Test ë¶„í• 
        self.split_data = self._split_data(split_ratio)
        
        # í‘œì¤€í™”
        if normalize and split == 'train':
            self.scaler_A = StandardScaler()
            self.scaler_B = StandardScaler()
            self._fit_scalers()
        elif normalize:
            self.scaler_A = None
            self.scaler_B = None
        
        # ìœˆë„ìš° ìƒì„±
        self.windows = self._create_windows()
    
    def _init_text_encoder(self):
        """Text encoder ì´ˆê¸°í™”"""
        if self.text_mode == 'bert':
            try:
                from transformers import AutoTokenizer, AutoModel
                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
                self.text_model = AutoModel.from_pretrained('bert-base-uncased')
                self.text_model.eval()
                print("âœ“ BERT encoder loaded")
            except:
                print("âš  BERT not available, falling back to simple mode")
                self.text_mode = 'simple'
        
        elif self.text_mode == 'sentence-transformer':
            try:
                from sentence_transformers import SentenceTransformer
                self.text_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ“ Sentence-Transformer encoder loaded")
            except:
                print("âš  Sentence-Transformer not available, falling back to simple mode")
                self.text_mode = 'simple'
    
    def _process_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if self.text_mode == 'simple':
            # ë‹¨ìˆœ í†µê³„: ê¸¸ì´, ë‹¨ì–´ ìˆ˜, ë¬¸ì¥ ìˆ˜
            return np.array([
                len(text), 
                text.count(' '),
                text.count('.') + text.count('!') + text.count('?')
            ])
        
        elif self.text_mode == 'bert':
            # BERT embedding
            with torch.no_grad():
                inputs = self.tokenizer(text, return_tensors='pt', 
                                       truncation=True, max_length=512)
                outputs = self.text_model(**inputs)
                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            return embedding
        
        elif self.text_mode == 'sentence-transformer':
            # Sentence embedding
            return self.text_model.encode(text)
    
    def _load_data(self, root_dir, domain):
        """Numericalê³¼ Textual ë°ì´í„° ë¡œë“œ"""
        # Numerical data
        num_path = Path(root_dir) / 'numerical' / domain / f'{domain}.csv'
        num_df = pd.read_csv(num_path)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ (date ìš°ì„ , ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì»¬ëŸ¼)
        date_col = self._find_date_column(num_df)
        num_df['date'] = pd.to_datetime(num_df[date_col])
        num_df = num_df.sort_values('date')
        
        # Textual data (search ì‚¬ìš©)
        text_path = Path(root_dir) / 'textual' / domain / f'{domain}_search.csv'
        text_df = pd.read_csv(text_path)
        text_df['start_date'] = pd.to_datetime(text_df['start_date'])
        text_df['end_date'] = pd.to_datetime(text_df['end_date'])
        text_df = text_df.sort_values('start_date')
        
        return num_df, text_df
    
    def _find_date_column(self, df):
        """ë‚ ì§œ ì»¬ëŸ¼ ìë™ ì°¾ê¸°"""
        # ìš°ì„ ìˆœìœ„: date > Date > Month > MapDate > ì²« ë²ˆì§¸ ì»¬ëŸ¼
        possible_date_cols = ['date', 'Date', 'Month', 'MapDate']
        
        for col in possible_date_cols:
            if col in df.columns:
                return col
        
        # ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
        print(f"âš  Warning: No standard date column found. Using first column: {df.columns[0]}")
        return df.columns[0]
    
    def _align_timeseries(self):
        """ì‹œê°„ì¶•ì„ ì •ë ¬í•˜ì—¬ ê°™ì€ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë§¤ì¹­"""
        num_dates = pd.to_datetime(self.numerical_data['date'])
        text_dates = pd.to_datetime(self.textual_data['start_date'])
        
        all_dates = pd.date_range(
            start=max(num_dates.min(), text_dates.min()),
            end=min(num_dates.max(), text_dates.max()),
            freq='MS'
        )
        
        aligned = []
        for date in all_dates:
            # Numerical ë°ì´í„°
            num_row = self.numerical_data[self.numerical_data['date'] == date]
            if len(num_row) > 0:
                num_values = num_row.select_dtypes(include=[np.number]).values.flatten()
                mask_A = 1
            else:
                num_values = np.zeros(self._get_num_features())
                mask_A = 0
            
            # Textual ë°ì´í„°
            text_row = self.textual_data[
                (self.textual_data['start_date'] <= date) & 
                (self.textual_data['end_date'] >= date)
            ]
            
            if len(text_row) > 0:
                fact = str(text_row.iloc[0]['fact'])
                # 'nan', 'NA', ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                if fact in ['nan', 'NA', '', 'None']:
                    text_values = self._process_text("")
                    mask_B = 0
                    raw_text = "[NO TEXT]"
                else:
                    text_values = self._process_text(fact)
                    mask_B = 1
                    raw_text = fact
            else:
                text_values = self._process_text("")
                mask_B = 0
                raw_text = "[NO TEXT]"
            
            aligned.append({
                'date': date,
                'xA': num_values,
                'xB': text_values,
                'maskA': mask_A,
                'maskB': mask_B,
                'text': raw_text  # âœ¨ ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥
            })
        
        return aligned
    
    def _get_num_features(self):
        """Numerical feature ê°œìˆ˜"""
        return len(self.numerical_data.select_dtypes(include=[np.number]).columns)
    
    def _split_data(self, split_ratio):
        """ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í• """
        n = len(self.aligned_data)
        train_end = int(n * split_ratio[0])
        val_end = train_end + int(n * split_ratio[1])
        
        if self.split == 'train':
            return self.aligned_data[:train_end]
        elif self.split == 'val':
            return self.aligned_data[train_end:val_end]
        else:
            return self.aligned_data[val_end:]
    
    def _fit_scalers(self):
        """Train ë°ì´í„°ë¡œ scaler fit"""
        xA_all = np.array([d['xA'] for d in self.split_data])
        xB_all = np.array([d['xB'] for d in self.split_data])
        
        maskA_all = np.array([d['maskA'] for d in self.split_data])
        maskB_all = np.array([d['maskB'] for d in self.split_data])
        
        if maskA_all.sum() > 0:
            self.scaler_A.fit(xA_all[maskA_all == 1])
        if maskB_all.sum() > 0:
            self.scaler_B.fit(xB_all[maskB_all == 1])
    
    def _create_windows(self):
        """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±"""
        windows = []
        n = len(self.split_data)
        
        for i in range(0, n - self.window_size + 1, self.stride):
            window = self.split_data[i:i + self.window_size]
            windows.append(window)
        
        return windows
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        window = self.windows[idx]
        
        xA = np.array([d['xA'] for d in window], dtype=np.float32)
        xB = np.array([d['xB'] for d in window], dtype=np.float32)
        maskA = np.array([d['maskA'] for d in window], dtype=np.float32)
        maskB = np.array([d['maskB'] for d in window], dtype=np.float32)
        
        # ì›ë³¸ ì €ì¥ (í‘œì¤€í™” ì „)
        xA_raw = xA.copy()
        
        # í‘œì¤€í™”
        if self.normalize and self.scaler_A is not None:
            valid_A = maskA == 1
            if valid_A.sum() > 0:
                xA[valid_A] = self.scaler_A.transform(xA[valid_A])
        
        if self.normalize and self.scaler_B is not None:
            valid_B = maskB == 1
            if valid_B.sum() > 0:
                xB[valid_B] = self.scaler_B.transform(xB[valid_B])
        
        result = {
            'xA': torch.FloatTensor(xA),
            'xB': torch.FloatTensor(xB),
            'maskA': torch.FloatTensor(maskA),
            'maskB': torch.FloatTensor(maskB),
        }
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if self.return_metadata:
            # pandas Timestampë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (DataLoader í˜¸í™˜)
            result['dates'] = [str(d['date']) for d in window]
            result['texts'] = [d['text'] for d in window]
            result['xA_raw'] = torch.FloatTensor(xA_raw)
        
        return result
    
    def verify_alignment(self, idx=0, n_steps=5):
        """ë°ì´í„° ì •ë ¬ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
        batch = self[idx]
        
        print("\n" + "=" * 80)
        print(f"ğŸ“Š Sample {idx} Alignment Verification")
        print("=" * 80)
        
        n_steps = min(n_steps, len(batch['dates']))
        
        for t in range(n_steps):
            print(f"\n[â° Time Step {t}]")
            print(f"  ğŸ“… Date: {batch['dates'][t]}")
            print(f"  ğŸ“ Text: {batch['texts'][t][:150]}..." if len(batch['texts'][t]) > 150 else f"  ğŸ“ Text: {batch['texts'][t]}")
            print(f"  ğŸ”¢ Numerical (first 3): {batch['xA_raw'][t][:3]}")
            print(f"  ğŸ“Š Text features: {batch['xB'][t][:5]}...")
            print(f"  âœ… Masks: Numerical={batch['maskA'][t].item()}, Text={batch['maskB'][t].item()}")
        
        print("\n" + "=" * 80)
    
    def get_scalers(self):
        """Scaler ë°˜í™˜"""
        return self.scaler_A, self.scaler_B
    
    def set_scalers(self, scaler_A, scaler_B):
        """Scaler ì„¤ì •"""
        self.scaler_A = scaler_A
        self.scaler_B = scaler_B

